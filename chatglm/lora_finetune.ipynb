{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-05T10:49:46.530046Z",
     "iopub.status.busy": "2024-04-05T10:49:46.529702Z",
     "iopub.status.idle": "2024-04-05T10:49:47.868626Z",
     "shell.execute_reply": "2024-04-05T10:49:47.868106Z",
     "shell.execute_reply.started": "2024-04-05T10:49:46.530028Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false,
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/media/zr/Data/Code/ChatGLM3/venv/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-05T11:23:24.733554Z",
     "iopub.status.busy": "2024-04-05T11:23:24.733210Z",
     "iopub.status.idle": "2024-04-05T12:26:52.057671Z",
     "shell.execute_reply": "2024-04-05T12:26:52.057005Z",
     "shell.execute_reply.started": "2024-04-05T11:23:24.733536Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  3.38it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:00<00:00, 1558.00 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.8309, 'grad_norm': 2.2304773330688477, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6031, 'grad_norm': 3.1985549926757812, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 4.491, 'grad_norm': 3.0141987800598145, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1264, 'grad_norm': 3.4274418354034424, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1193, 'grad_norm': 2.7462944984436035, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.0}\n",
      "{'loss': 3.866, 'grad_norm': 2.990652322769165, 'learning_rate': 4.9e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8418, 'grad_norm': 2.8835508823394775, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7465, 'grad_norm': 2.9447004795074463, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6381, 'grad_norm': 3.2199761867523193, 'learning_rate': 4.85e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7207, 'grad_norm': 3.4081695079803467, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.673, 'grad_norm': 3.6430819034576416, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8488, 'grad_norm': 3.857043981552124, 'learning_rate': 4.8e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6146, 'grad_norm': 3.477768659591675, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7311, 'grad_norm': 4.40957498550415, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6838, 'grad_norm': 3.6516358852386475, 'learning_rate': 4.75e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7441, 'grad_norm': 3.916576623916626, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.576, 'grad_norm': 4.074813365936279, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5771, 'grad_norm': 4.284194469451904, 'learning_rate': 4.7e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5521, 'grad_norm': 4.779750823974609, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5791, 'grad_norm': 4.499788284301758, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.551, 'grad_norm': 4.8730058670043945, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.648, 'grad_norm': 4.111719131469727, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6102, 'grad_norm': 4.695887088775635, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5111, 'grad_norm': 4.526453971862793, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4762, 'grad_norm': 5.41154670715332, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6006, 'grad_norm': 5.232150077819824, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5484, 'grad_norm': 5.3777852058410645, 'learning_rate': 4.55e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6135, 'grad_norm': 4.497265338897705, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6285, 'grad_norm': 4.74179744720459, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5398, 'grad_norm': 5.8801679611206055, 'learning_rate': 4.5e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4684, 'grad_norm': 5.2244391441345215, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6072, 'grad_norm': 5.771844863891602, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4166, 'grad_norm': 5.264108657836914, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.493, 'grad_norm': 5.401803493499756, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5186, 'grad_norm': 5.523139953613281, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5752, 'grad_norm': 5.28311014175415, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3588, 'grad_norm': 4.840222358703613, 'learning_rate': 4.383333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5287, 'grad_norm': 5.108975887298584, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5229, 'grad_norm': 5.251105308532715, 'learning_rate': 4.35e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4711, 'grad_norm': 5.619790554046631, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6924, 'grad_norm': 5.497077465057373, 'learning_rate': 4.316666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.499, 'grad_norm': 5.004016399383545, 'learning_rate': 4.3e-05, 'epoch': 0.01}\n",
      "{'loss': 3.626, 'grad_norm': 5.650815963745117, 'learning_rate': 4.2833333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4203, 'grad_norm': 6.519309043884277, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4143, 'grad_norm': 6.068180561065674, 'learning_rate': 4.25e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4279, 'grad_norm': 5.621462345123291, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.533, 'grad_norm': 5.694795608520508, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4465, 'grad_norm': 7.043788909912109, 'learning_rate': 4.2e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4602, 'grad_norm': 5.81734037399292, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5619, 'grad_norm': 5.93455696105957, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.02}\n",
      " 17%|██████▋                                 | 500/3000 [07:00<39:52,  1.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:24<00:24, 12.34s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:30<00:09,  9.44s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:48<00:00, 12.94s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.567 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 31.849876000000005, 'eval_rouge-2': 7.289712000000001, 'eval_rouge-l': 24.592048000000005, 'eval_bleu-4': 0.033329628522891105, 'eval_runtime': 53.3761, 'eval_samples_per_second': 0.937, 'eval_steps_per_second': 0.075, 'epoch': 0.02}\n",
      " 17%|██████▋                                 | 500/3000 [07:54<39:52,  1.05it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:49<00:00, 12.94s/it]\u001b[A\n",
      "{'loss': 3.3223, 'grad_norm': 5.778697967529297, 'learning_rate': 4.15e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5439, 'grad_norm': 6.696754455566406, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.584, 'grad_norm': 6.051486492156982, 'learning_rate': 4.116666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4871, 'grad_norm': 5.425057888031006, 'learning_rate': 4.1e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5232, 'grad_norm': 5.372986793518066, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6439, 'grad_norm': 5.855948448181152, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4936, 'grad_norm': 5.847175121307373, 'learning_rate': 4.05e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3775, 'grad_norm': 5.651453018188477, 'learning_rate': 4.0333333333333336e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4254, 'grad_norm': 6.229981422424316, 'learning_rate': 4.016666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4947, 'grad_norm': 6.487583160400391, 'learning_rate': 4e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4408, 'grad_norm': 6.396424770355225, 'learning_rate': 3.983333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.457, 'grad_norm': 6.714562892913818, 'learning_rate': 3.966666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4465, 'grad_norm': 5.994319915771484, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4617, 'grad_norm': 6.331824779510498, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5334, 'grad_norm': 5.918545722961426, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4859, 'grad_norm': 6.4406938552856445, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5432, 'grad_norm': 6.2590484619140625, 'learning_rate': 3.883333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.302, 'grad_norm': 7.261928558349609, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3992, 'grad_norm': 6.745319366455078, 'learning_rate': 3.85e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3563, 'grad_norm': 6.282355785369873, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4994, 'grad_norm': 7.08056640625, 'learning_rate': 3.816666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5264, 'grad_norm': 6.7908759117126465, 'learning_rate': 3.8e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2488, 'grad_norm': 6.997071743011475, 'learning_rate': 3.7833333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5771, 'grad_norm': 5.889676570892334, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3979, 'grad_norm': 6.577539443969727, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4777, 'grad_norm': 6.165267467498779, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6199, 'grad_norm': 6.421123027801514, 'learning_rate': 3.7166666666666664e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4736, 'grad_norm': 6.2777814865112305, 'learning_rate': 3.7e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3256, 'grad_norm': 6.589746475219727, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5531, 'grad_norm': 7.009218215942383, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2932, 'grad_norm': 6.555532455444336, 'learning_rate': 3.65e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3594, 'grad_norm': 6.522977828979492, 'learning_rate': 3.633333333333333e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4623, 'grad_norm': 7.212965488433838, 'learning_rate': 3.6166666666666674e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4086, 'grad_norm': 6.332172393798828, 'learning_rate': 3.6e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5045, 'grad_norm': 6.313897609710693, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5395, 'grad_norm': 6.368630409240723, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2943, 'grad_norm': 7.153632164001465, 'learning_rate': 3.55e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4947, 'grad_norm': 6.732903003692627, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4514, 'grad_norm': 7.6350202560424805, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2652, 'grad_norm': 7.849453449249268, 'learning_rate': 3.5e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4619, 'grad_norm': 7.834907531738281, 'learning_rate': 3.483333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4193, 'grad_norm': 7.026005744934082, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4623, 'grad_norm': 7.543944358825684, 'learning_rate': 3.45e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5676, 'grad_norm': 7.361222267150879, 'learning_rate': 3.433333333333333e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3656, 'grad_norm': 6.515172958374023, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4369, 'grad_norm': 7.992982864379883, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.03}\n",
      "{'loss': 3.533, 'grad_norm': 5.983039379119873, 'learning_rate': 3.3833333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3264, 'grad_norm': 7.040143966674805, 'learning_rate': 3.366666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4574, 'grad_norm': 7.32929801940918, 'learning_rate': 3.35e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3951, 'grad_norm': 7.986578941345215, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.03}\n",
      " 33%|█████████████                          | 1000/3000 [14:49<28:53,  1.15it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.70s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:06<00:02,  2.35s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.643886000000002, 'eval_rouge-2': 6.818972000000001, 'eval_rouge-l': 25.822108, 'eval_bleu-4': 0.03342167968750986, 'eval_runtime': 12.6888, 'eval_samples_per_second': 3.94, 'eval_steps_per_second': 0.315, 'epoch': 0.03}\n",
      " 33%|█████████████                          | 1000/3000 [15:02<28:53,  1.15it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:09<00:00,  2.39s/it]\u001b[A\n",
      "{'loss': 3.448, 'grad_norm': 6.964615821838379, 'learning_rate': 3.316666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4582, 'grad_norm': 7.485027313232422, 'learning_rate': 3.3e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6498, 'grad_norm': 8.2475004196167, 'learning_rate': 3.283333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4049, 'grad_norm': 6.643664360046387, 'learning_rate': 3.266666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3906, 'grad_norm': 8.769279479980469, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3588, 'grad_norm': 7.829706192016602, 'learning_rate': 3.233333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3885, 'grad_norm': 7.244487762451172, 'learning_rate': 3.2166666666666665e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4643, 'grad_norm': 7.320113658905029, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5293, 'grad_norm': 7.267946243286133, 'learning_rate': 3.183333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4674, 'grad_norm': 6.735072135925293, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.04}\n",
      "{'loss': 3.351, 'grad_norm': 6.898447513580322, 'learning_rate': 3.15e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5293, 'grad_norm': 7.981277942657471, 'learning_rate': 3.1333333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4324, 'grad_norm': 7.407705307006836, 'learning_rate': 3.116666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3621, 'grad_norm': 8.195934295654297, 'learning_rate': 3.1e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3207, 'grad_norm': 7.651887893676758, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3609, 'grad_norm': 7.191196918487549, 'learning_rate': 3.066666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4553, 'grad_norm': 6.801367282867432, 'learning_rate': 3.05e-05, 'epoch': 0.04}\n",
      "{'loss': 3.473, 'grad_norm': 6.4806904792785645, 'learning_rate': 3.0333333333333337e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3582, 'grad_norm': 6.7136993408203125, 'learning_rate': 3.016666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4129, 'grad_norm': 6.417901515960693, 'learning_rate': 3e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2412, 'grad_norm': 6.703406810760498, 'learning_rate': 2.9833333333333335e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3416, 'grad_norm': 7.359005451202393, 'learning_rate': 2.9666666666666672e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3816, 'grad_norm': 7.513006687164307, 'learning_rate': 2.95e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3783, 'grad_norm': 7.457762241363525, 'learning_rate': 2.9333333333333336e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4479, 'grad_norm': 6.8869757652282715, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.283, 'grad_norm': 7.731271743774414, 'learning_rate': 2.9e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4557, 'grad_norm': 7.241689682006836, 'learning_rate': 2.8833333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3385, 'grad_norm': 7.290231704711914, 'learning_rate': 2.8666666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3877, 'grad_norm': 7.0788750648498535, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4854, 'grad_norm': 7.510239124298096, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4613, 'grad_norm': 7.085663795471191, 'learning_rate': 2.816666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4535, 'grad_norm': 6.7950005531311035, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4039, 'grad_norm': 10.692102432250977, 'learning_rate': 2.7833333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3057, 'grad_norm': 7.541746139526367, 'learning_rate': 2.7666666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3498, 'grad_norm': 7.702714920043945, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3006, 'grad_norm': 7.9808855056762695, 'learning_rate': 2.733333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.5197, 'grad_norm': 7.3960747718811035, 'learning_rate': 2.716666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3861, 'grad_norm': 7.194422245025635, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3572, 'grad_norm': 7.105179786682129, 'learning_rate': 2.6833333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4186, 'grad_norm': 6.670580863952637, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3531, 'grad_norm': 7.681300640106201, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2674, 'grad_norm': 7.772520065307617, 'learning_rate': 2.633333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3863, 'grad_norm': 7.834169387817383, 'learning_rate': 2.6166666666666668e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3611, 'grad_norm': 7.330804347991943, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2727, 'grad_norm': 6.94488000869751, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3959, 'grad_norm': 7.311525344848633, 'learning_rate': 2.5666666666666666e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4393, 'grad_norm': 9.915614128112793, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3035, 'grad_norm': 6.905265808105469, 'learning_rate': 2.5333333333333337e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4404, 'grad_norm': 7.497659206390381, 'learning_rate': 2.5166666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4613, 'grad_norm': 6.963030815124512, 'learning_rate': 2.5e-05, 'epoch': 0.05}\n",
      " 50%|███████████████████▌                   | 1500/3000 [21:56<18:27,  1.35it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:06<00:06,  3.23s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:10<00:03,  3.38s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.253196, 'eval_rouge-2': 7.293970000000001, 'eval_rouge-l': 24.859157999999997, 'eval_bleu-4': 0.03660125808784014, 'eval_runtime': 37.8, 'eval_samples_per_second': 1.323, 'eval_steps_per_second': 0.106, 'epoch': 0.05}\n",
      " 50%|███████████████████▌                   | 1500/3000 [22:34<18:27,  1.35it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:12<00:00,  3.11s/it]\u001b[A\n",
      "{'loss': 3.343, 'grad_norm': 6.9597601890563965, 'learning_rate': 2.4833333333333335e-05, 'epoch': 0.05}\n",
      "{'loss': 3.385, 'grad_norm': 8.143205642700195, 'learning_rate': 2.466666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4414, 'grad_norm': 8.3147611618042, 'learning_rate': 2.45e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4043, 'grad_norm': 7.140670299530029, 'learning_rate': 2.4333333333333336e-05, 'epoch': 0.05}\n",
      "{'loss': 3.499, 'grad_norm': 7.580584526062012, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4104, 'grad_norm': 8.37860107421875, 'learning_rate': 2.4e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4721, 'grad_norm': 8.074492454528809, 'learning_rate': 2.3833333333333334e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4398, 'grad_norm': 7.609441757202148, 'learning_rate': 2.3666666666666668e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5137, 'grad_norm': 9.597918510437012, 'learning_rate': 2.35e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3916, 'grad_norm': 7.318232536315918, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3682, 'grad_norm': 8.095166206359863, 'learning_rate': 2.3166666666666666e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3713, 'grad_norm': 8.51609992980957, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4756, 'grad_norm': 7.4402971267700195, 'learning_rate': 2.2833333333333334e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3219, 'grad_norm': 8.10599422454834, 'learning_rate': 2.2666666666666668e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3697, 'grad_norm': 7.644421577453613, 'learning_rate': 2.25e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3045, 'grad_norm': 7.181853294372559, 'learning_rate': 2.2333333333333335e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4803, 'grad_norm': 8.674893379211426, 'learning_rate': 2.216666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3742, 'grad_norm': 7.343319416046143, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.06}\n",
      "{'loss': 3.375, 'grad_norm': 7.418705463409424, 'learning_rate': 2.1833333333333333e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5162, 'grad_norm': 7.073418140411377, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4635, 'grad_norm': 7.469015121459961, 'learning_rate': 2.15e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5063, 'grad_norm': 7.534095764160156, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4047, 'grad_norm': 7.439194202423096, 'learning_rate': 2.116666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3994, 'grad_norm': 7.774364948272705, 'learning_rate': 2.1e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4703, 'grad_norm': 7.573234558105469, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.06}\n",
      "{'loss': 3.448, 'grad_norm': 7.991424083709717, 'learning_rate': 2.0666666666666666e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3625, 'grad_norm': 8.596370697021484, 'learning_rate': 2.05e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3523, 'grad_norm': 8.25791072845459, 'learning_rate': 2.0333333333333334e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3934, 'grad_norm': 8.282937049865723, 'learning_rate': 2.0166666666666668e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3387, 'grad_norm': 7.890267372131348, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3791, 'grad_norm': 9.121062278747559, 'learning_rate': 1.9833333333333335e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3408, 'grad_norm': 7.675883769989014, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5805, 'grad_norm': 7.991442680358887, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3469, 'grad_norm': 8.67477035522461, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4969, 'grad_norm': 9.201820373535156, 'learning_rate': 1.9166666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3801, 'grad_norm': 7.544164657592773, 'learning_rate': 1.9e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3195, 'grad_norm': 8.315423965454102, 'learning_rate': 1.8833333333333335e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3053, 'grad_norm': 7.828063488006592, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4039, 'grad_norm': 7.356893062591553, 'learning_rate': 1.85e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3752, 'grad_norm': 8.070150375366211, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3904, 'grad_norm': 8.097166061401367, 'learning_rate': 1.8166666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4801, 'grad_norm': 7.481112003326416, 'learning_rate': 1.8e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2818, 'grad_norm': 7.880654335021973, 'learning_rate': 1.7833333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.501, 'grad_norm': 7.6629638671875, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3643, 'grad_norm': 6.832037448883057, 'learning_rate': 1.75e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2869, 'grad_norm': 8.856221199035645, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3723, 'grad_norm': 7.640097618103027, 'learning_rate': 1.7166666666666666e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2416, 'grad_norm': 7.656063556671143, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4186, 'grad_norm': 7.244184494018555, 'learning_rate': 1.6833333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4674, 'grad_norm': 8.0363130569458, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.07}\n",
      " 67%|██████████████████████████             | 2000/3000 [29:29<13:29,  1.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.10s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.62s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.522888000000002, 'eval_rouge-2': 6.9905740000000005, 'eval_rouge-l': 24.618073999999996, 'eval_bleu-4': 0.035426559355337284, 'eval_runtime': 35.1998, 'eval_samples_per_second': 1.42, 'eval_steps_per_second': 0.114, 'epoch': 0.07}\n",
      " 67%|██████████████████████████             | 2000/3000 [30:04<13:29,  1.24it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:10<00:00,  2.60s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2000\n",
      "loading configuration file config.json from cache at /home/ec2-user/llm/hf/hub/models--THUDM--chatglm3-6b/snapshots/103caa40027ebfd8450289ca2f278eac4ff26405/config.json\n",
      "Model config ChatGLMConfig {\n",
      "  \"_name_or_path\": \"THUDM/chatglm3-6b\",\n",
      "  \"add_bias_linear\": false,\n",
      "  \"add_qkv_bias\": true,\n",
      "  \"apply_query_key_layer_scaling\": true,\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"ChatGLMModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig\",\n",
      "    \"AutoModel\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForCausalLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSeq2SeqLM\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
      "    \"AutoModelForSequenceClassification\": \"THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification\"\n",
      "  },\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ffn_hidden_size\": 13696,\n",
      "  \"fp32_residual_connection\": false,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layernorm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"chatglm\",\n",
      "  \"multi_query_attention\": true,\n",
      "  \"multi_query_group_num\": 2,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_layers\": 28,\n",
      "  \"original_rope\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"padded_vocab_size\": 65024,\n",
      "  \"post_layer_norm\": true,\n",
      "  \"pre_seq_len\": null,\n",
      "  \"prefix_projection\": false,\n",
      "  \"quantization_bit\": 0,\n",
      "  \"rmsnorm\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 65024\n",
      "}\n",
      "\n",
      "{'loss': 3.3908, 'grad_norm': 8.901200294494629, 'learning_rate': 1.65e-05, 'epoch': 0.07}\n",
      "{'loss': 3.498, 'grad_norm': 7.732595920562744, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5598, 'grad_norm': 8.985759735107422, 'learning_rate': 1.6166666666666665e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4922, 'grad_norm': 8.433785438537598, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3725, 'grad_norm': 8.285409927368164, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3318, 'grad_norm': 7.926369667053223, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4406, 'grad_norm': 8.178301811218262, 'learning_rate': 1.55e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4199, 'grad_norm': 8.310633659362793, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4414, 'grad_norm': 7.545776844024658, 'learning_rate': 1.5166666666666668e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3586, 'grad_norm': 7.779127597808838, 'learning_rate': 1.5e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2979, 'grad_norm': 7.738952159881592, 'learning_rate': 1.4833333333333336e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5871, 'grad_norm': 8.1212739944458, 'learning_rate': 1.4666666666666668e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2504, 'grad_norm': 7.699244976043701, 'learning_rate': 1.45e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3621, 'grad_norm': 8.38377571105957, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4012, 'grad_norm': 7.497560024261475, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5178, 'grad_norm': 8.219110488891602, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3928, 'grad_norm': 7.0344929695129395, 'learning_rate': 1.3833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4166, 'grad_norm': 8.041406631469727, 'learning_rate': 1.3666666666666666e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3504, 'grad_norm': 7.724899768829346, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4373, 'grad_norm': 7.761056900024414, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4469, 'grad_norm': 7.030317306518555, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4219, 'grad_norm': 7.995706558227539, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4205, 'grad_norm': 8.167654037475586, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3781, 'grad_norm': 8.323127746582031, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2385, 'grad_norm': 8.462663650512695, 'learning_rate': 1.25e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3617, 'grad_norm': 8.083922386169434, 'learning_rate': 1.2333333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.432, 'grad_norm': 8.809940338134766, 'learning_rate': 1.2166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4674, 'grad_norm': 7.805532455444336, 'learning_rate': 1.2e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2938, 'grad_norm': 8.461136817932129, 'learning_rate': 1.1833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3602, 'grad_norm': 8.47400188446045, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3199, 'grad_norm': 8.464133262634277, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3273, 'grad_norm': 8.520477294921875, 'learning_rate': 1.1333333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3742, 'grad_norm': 9.07942008972168, 'learning_rate': 1.1166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3652, 'grad_norm': 7.910879135131836, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2723, 'grad_norm': 8.833725929260254, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3818, 'grad_norm': 8.517478942871094, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3545, 'grad_norm': 7.999580383300781, 'learning_rate': 1.05e-05, 'epoch': 0.08}\n",
      "{'loss': 3.498, 'grad_norm': 8.694116592407227, 'learning_rate': 1.0333333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2371, 'grad_norm': 8.632969856262207, 'learning_rate': 1.0166666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4568, 'grad_norm': 7.866032600402832, 'learning_rate': 1e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4611, 'grad_norm': 8.326533317565918, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.08}\n",
      "{'loss': 3.2812, 'grad_norm': 8.236929893493652, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.08}\n",
      "{'loss': 3.3713, 'grad_norm': 7.351326942443848, 'learning_rate': 9.5e-06, 'epoch': 0.08}\n",
      "{'loss': 3.3826, 'grad_norm': 8.090866088867188, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2744, 'grad_norm': 7.8559064865112305, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.09}\n",
      "{'loss': 3.316, 'grad_norm': 7.883625030517578, 'learning_rate': 9e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2613, 'grad_norm': 8.701327323913574, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4393, 'grad_norm': 7.5172119140625, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4785, 'grad_norm': 7.8609395027160645, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3986, 'grad_norm': 9.430017471313477, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.09}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [36:58<06:45,  1.23it/s]/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:04<00:04,  2.27s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:07<00:02,  2.58s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.750428000000003, 'eval_rouge-2': 6.9816139999999995, 'eval_rouge-l': 23.939379999999996, 'eval_bleu-4': 0.03239959911129344, 'eval_runtime': 50.7194, 'eval_samples_per_second': 0.986, 'eval_steps_per_second': 0.079, 'epoch': 0.09}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [37:49<06:45,  1.23it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:25<00:00,  8.35s/it]\u001b[A\n",
      "{'loss': 3.3033, 'grad_norm': 8.646208763122559, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3404, 'grad_norm': 10.074450492858887, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2467, 'grad_norm': 7.9628682136535645, 'learning_rate': 7.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3984, 'grad_norm': 8.44014835357666, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3932, 'grad_norm': 7.7957305908203125, 'learning_rate': 7.5e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4041, 'grad_norm': 8.49970817565918, 'learning_rate': 7.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4766, 'grad_norm': 8.074983596801758, 'learning_rate': 7.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4859, 'grad_norm': 8.671357154846191, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3744, 'grad_norm': 8.707027435302734, 'learning_rate': 6.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4809, 'grad_norm': 8.936946868896484, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3584, 'grad_norm': 8.156333923339844, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4268, 'grad_norm': 7.748749256134033, 'learning_rate': 6.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.526, 'grad_norm': 7.655928611755371, 'learning_rate': 6.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4477, 'grad_norm': 8.66393756866455, 'learning_rate': 6e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4072, 'grad_norm': 8.166670799255371, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3563, 'grad_norm': 8.134453773498535, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4133, 'grad_norm': 8.87308406829834, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2701, 'grad_norm': 7.60411262512207, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4752, 'grad_norm': 8.889498710632324, 'learning_rate': 5.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4506, 'grad_norm': 9.17124080657959, 'learning_rate': 5e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4256, 'grad_norm': 8.634198188781738, 'learning_rate': 4.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2508, 'grad_norm': 7.654784202575684, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3805, 'grad_norm': 8.044384002685547, 'learning_rate': 4.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3879, 'grad_norm': 8.150736808776855, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4572, 'grad_norm': 9.0230712890625, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4006, 'grad_norm': 8.30868911743164, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3488, 'grad_norm': 8.311358451843262, 'learning_rate': 3.833333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2623, 'grad_norm': 8.669110298156738, 'learning_rate': 3.666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.284, 'grad_norm': 8.193401336669922, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2473, 'grad_norm': 7.943253040313721, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4502, 'grad_norm': 7.926661968231201, 'learning_rate': 3.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3787, 'grad_norm': 8.194805145263672, 'learning_rate': 3e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3904, 'grad_norm': 8.152848243713379, 'learning_rate': 2.8333333333333335e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4465, 'grad_norm': 9.1334810256958, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4061, 'grad_norm': 8.691346168518066, 'learning_rate': 2.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3391, 'grad_norm': 8.521377563476562, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3746, 'grad_norm': 8.774834632873535, 'learning_rate': 2.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.5123, 'grad_norm': 9.365562438964844, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3062, 'grad_norm': 8.278939247131348, 'learning_rate': 1.8333333333333335e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3307, 'grad_norm': 9.04343318939209, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3068, 'grad_norm': 7.969751358032227, 'learning_rate': 1.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2502, 'grad_norm': 7.4023566246032715, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3664, 'grad_norm': 9.016193389892578, 'learning_rate': 1.1666666666666668e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2598, 'grad_norm': 8.3810453414917, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3813, 'grad_norm': 8.1025390625, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.1}\n",
      "{'loss': 3.2096, 'grad_norm': 9.029080390930176, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.1}\n",
      "{'loss': 3.4492, 'grad_norm': 8.935340881347656, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.1}\n",
      "{'loss': 3.4299, 'grad_norm': 8.96049690246582, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.1}\n",
      "{'loss': 3.4744, 'grad_norm': 8.077353477478027, 'learning_rate': 1.6666666666666668e-07, 'epoch': 0.1}\n",
      "{'loss': 3.3678, 'grad_norm': 7.941110134124756, 'learning_rate': 0.0, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [44:45<00:00,  1.28it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:24<00:24, 12.34s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:28<00:08,  8.61s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.667582, 'eval_rouge-2': 7.568654, 'eval_rouge-l': 24.778176, 'eval_bleu-4': 0.03565805858375573, 'eval_runtime': 71.2592, 'eval_samples_per_second': 0.702, 'eval_steps_per_second': 0.056, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [45:56<00:00,  1.28it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:46<00:00, 12.13s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2756.7509, 'train_samples_per_second': 4.353, 'train_steps_per_second': 1.088, 'train_loss': 3.4475911458333335, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [45:56<00:00,  1.09it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [16:53<00:00, 15.12s/it]\n"
     ]
    }
   ],
   "source": [
    "!/opt/conda/envs/myenv/bin/python finetune_hf.py  data/AdvertiseGen_fix THUDM/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-05T12:26:52.059451Z",
     "iopub.status.busy": "2024-04-05T12:26:52.058976Z",
     "iopub.status.idle": "2024-04-05T12:26:52.345440Z",
     "shell.execute_reply": "2024-04-05T12:26:52.344877Z",
     "shell.execute_reply.started": "2024-04-05T12:26:52.059427Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-2000\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-04-05T12:27:19.040921Z",
     "iopub.status.busy": "2024-04-05T12:27:19.040570Z",
     "iopub.status.idle": "2024-04-05T12:27:30.932618Z",
     "shell.execute_reply": "2024-04-05T12:27:30.932033Z",
     "shell.execute_reply.started": "2024-04-05T12:27:19.040902Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  3.02it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "这款连衣裙的拼接设计，在视觉上拉长了整体比例，搭配上压褶的百褶裙摆，使得整件连衣裙显得更加飘逸，穿起来更加优雅，更具有时尚感。再加上不规则的网纱抽褶设计，让整体造型更加时尚，还更具层次感，穿起来更显气质。经典的套头拉链设计，穿起来方便，不易脱线，穿着更加方便。\n"
     ]
    }
   ],
   "source": [
    "!/opt/conda/envs/myenv/bin/python inference_hf.py output/checkpoint-2000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
