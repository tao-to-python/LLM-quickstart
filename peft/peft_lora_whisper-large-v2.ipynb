{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219841f-493c-40f9-a6c9-3700f0c525d0",
   "metadata": {},
   "source": [
    "# PEFT 库 LoRA 实战 - OpenAI Whisper-large-v2\n",
    "\n",
    "本教程使用 LoRA 在`OpenAI Whisper-large-v2`模型上实现`语音识别(ASR)`任务的微调训练。\n",
    "\n",
    "我们还结合了`int8` 量化进一步降低训练过程资源开销，同时保证了精度几乎不受影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a1e23-ea71-45d6-82d6-453077cf2d29",
   "metadata": {},
   "source": [
    "## 全局参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccd00402-d821-485e-8703-fb16bcb56a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:35.718815Z",
     "iopub.status.busy": "2024-02-17T17:20:35.718354Z",
     "iopub.status.idle": "2024-02-17T17:20:35.722051Z",
     "shell.execute_reply": "2024-02-17T17:20:35.721448Z",
     "shell.execute_reply.started": "2024-02-17T17:20:35.718790Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad23c5-78c7-4d6b-84be-ceb1b5d3779c",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "### 下载数据集 Common Voice\n",
    "Common Voice 11.0 数据集包含许多不同语言的录音，总时长达数小时。\n",
    "\n",
    "本教程以中文数据为例，展示如何使用 LoRA 在 Whisper-large-v2 上进行微调训练。\n",
    "\n",
    "首先，初始化一个DatasetDict结构，并将训练集（将训练+验证拆分为训练集）和测试集拆分好，按照中文数据集构建配置加载到内存中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6ec239a-b2b0-4669-8d9b-07664487e4a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:37.744564Z",
     "iopub.status.busy": "2024-02-17T17:20:37.744136Z",
     "iopub.status.idle": "2024-02-17T17:20:39.727547Z",
     "shell.execute_reply": "2024-02-17T17:20:39.727030Z",
     "shell.execute_reply.started": "2024-02-17T17:20:37.744545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': '95368aab163e0387e4fd4991b4f2d8ccfbd4364bf656c860230501fd27dcedf087773e4695a6cf5de9c4f1d406d582283190d065cdfa36b0e2b060cffaca977e',\n",
       " 'path': '/home/ec2-user/llm/hf/datasets/downloads/extracted/36a149ee64d6d328278dadeb188a7ccf71af836d4dcbdb244f18ff2f123cd693/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       " 'audio': {'path': '/home/ec2-user/llm/hf/datasets/downloads/extracted/36a149ee64d6d328278dadeb188a7ccf71af836d4dcbdb244f18ff2f123cd693/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-9.09494702e-13, -2.50111043e-12, -2.04636308e-12, ...,\n",
       "          1.21667417e-05,  3.23003815e-06, -2.43064278e-07]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。',\n",
       " 'up_votes': 2,\n",
       " 'down_votes': 0,\n",
       " 'age': '',\n",
       " 'gender': '',\n",
       " 'accent': '',\n",
       " 'locale': 'zh-CN',\n",
       " 'segment': ''}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train\", trust_remote_code=True)\n",
    "common_voice[\"validation\"] = load_dataset(dataset_name, language_abbr, split=\"validation\", trust_remote_code=True)\n",
    "\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81faa4-d8fe-4cc7-afe6-4c2615b9050f",
   "metadata": {},
   "source": [
    "## 预处理训练数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5822025f-7f8e-4141-8bfe-d8822d0da20f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:41.225049Z",
     "iopub.status.busy": "2024-02-17T17:20:41.224511Z",
     "iopub.status.idle": "2024-02-17T17:20:42.560707Z",
     "shell.execute_reply": "2024-02-17T17:20:42.560132Z",
     "shell.execute_reply.started": "2024-02-17T17:20:41.225028Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# 从预训练模型加载特征提取器\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 从预训练模型加载分词器，可以指定语言和任务以获得最适合特定需求的分词器配置\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, language=language, task=task)\n",
    "\n",
    "# 从预训练模型加载处理器，处理器通常结合了特征提取器和分词器，为特定任务提供一站式的数据预处理\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394e5cd-23b8-413e-8bde-88c3542b84fa",
   "metadata": {},
   "source": [
    "#### 移除数据集中不必要的字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1690dc5a-c1f7-4556-9be3-d31ad888e52e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:42.671989Z",
     "iopub.status.busy": "2024-02-17T17:20:42.671642Z",
     "iopub.status.idle": "2024-02-17T17:20:42.677590Z",
     "shell.execute_reply": "2024-02-17T17:20:42.677045Z",
     "shell.execute_reply.started": "2024-02-17T17:20:42.671969Z"
    }
   },
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "309aff16-ea26-4474-af54-7ef244783999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:43.348470Z",
     "iopub.status.busy": "2024-02-17T17:20:43.347998Z",
     "iopub.status.idle": "2024-02-17T17:20:43.358532Z",
     "shell.execute_reply": "2024-02-17T17:20:43.358034Z",
     "shell.execute_reply.started": "2024-02-17T17:20:43.348450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/ec2-user/llm/hf/datasets/downloads/extracted/36a149ee64d6d328278dadeb188a7ccf71af836d4dcbdb244f18ff2f123cd693/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-9.09494702e-13, -2.50111043e-12, -2.04636308e-12, ...,\n",
       "          1.21667417e-05,  3.23003815e-06, -2.43064278e-07]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881546ab-72e4-4bcf-852f-a8be736164b7",
   "metadata": {},
   "source": [
    "#### 降采样音频数据\n",
    "\n",
    "查看`common_voice` 数据集介绍，你会发现其音频是以48kHz的采样率进行采样的.\n",
    "\n",
    "而`Whisper`模型是在16kHZ的音频输入上预训练的，因此我们需要将音频输入降采样以匹配模型预训练时使用的采样率。\n",
    "\n",
    "通过在音频列上使用`cast_column`方法，并将`sampling_rate`设置为16kHz来对音频进行降采样。\n",
    "\n",
    "下次调用时，音频输入将实时重新取样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fc451cc-e21e-473c-a702-d7d6ed098f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:46.835233Z",
     "iopub.status.busy": "2024-02-17T17:20:46.834618Z",
     "iopub.status.idle": "2024-02-17T17:20:46.840994Z",
     "shell.execute_reply": "2024-02-17T17:20:46.840436Z",
     "shell.execute_reply.started": "2024-02-17T17:20:46.835203Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc3d7fcc-7c34-41c8-9857-5a6e883f6115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:47.599997Z",
     "iopub.status.busy": "2024-02-17T17:20:47.599445Z",
     "iopub.status.idle": "2024-02-17T17:20:47.608970Z",
     "shell.execute_reply": "2024-02-17T17:20:47.608332Z",
     "shell.execute_reply.started": "2024-02-17T17:20:47.599977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/ec2-user/llm/hf/datasets/downloads/extracted/36a149ee64d6d328278dadeb188a7ccf71af836d4dcbdb244f18ff2f123cd693/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([ 6.54836185e-11, -2.91038305e-11, -5.82076609e-11, ...,\n",
       "         -5.96660539e-06,  2.71383760e-05,  1.29687833e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55908f-3ea3-4aee-8062-6f8d3a6573b9",
   "metadata": {},
   "source": [
    "### 整合以上数据处理为一个函数\n",
    "\n",
    "该数据预处理函数应该包括：\n",
    "- 通过加载音频列将音频输入重新采样为16kHZ。\n",
    "- 使用特征提取器从音频数组计算输入特征。\n",
    "- 将句子列标记化为输入标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f42c35-35ba-4d6b-9d15-095963cec67c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:49.500597Z",
     "iopub.status.busy": "2024-02-17T17:20:49.500237Z",
     "iopub.status.idle": "2024-02-17T17:20:49.504015Z",
     "shell.execute_reply": "2024-02-17T17:20:49.503385Z",
     "shell.execute_reply.started": "2024-02-17T17:20:49.500577Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a9f890-ab17-47f8-ada5-e256a327d2ef",
   "metadata": {},
   "source": [
    "## 数据抽样（演示需要）\n",
    "在 Whisper-Large-v2 上使用小规模数据进行演示训练，保持以下训练参数不变（batch_size=64）。\n",
    "\n",
    "使用 640 个样本训练，320个样本验证和评估，恰好使得1个 epoch 仅需10 steps 即可完成训练。\n",
    "\n",
    "（在 NVIDIA T4 上需要10-15分钟）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ea2cdfc-d1eb-4b4f-87f0-5df47cb99afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:51.614964Z",
     "iopub.status.busy": "2024-02-17T17:20:51.614604Z",
     "iopub.status.idle": "2024-02-17T17:20:51.624079Z",
     "shell.execute_reply": "2024-02-17T17:20:51.623578Z",
     "shell.execute_reply.started": "2024-02-17T17:20:51.614944Z"
    }
   },
   "outputs": [],
   "source": [
    "small_common_voice = DatasetDict()\n",
    "\n",
    "small_common_voice[\"train\"] = common_voice[\"train\"].shuffle(seed=16).select(range(640))\n",
    "small_common_voice[\"validation\"] = common_voice[\"validation\"].shuffle(seed=16).select(range(320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06ae5b53-d395-4469-9ee6-7ac1076987a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:54.535969Z",
     "iopub.status.busy": "2024-02-17T17:20:54.535535Z",
     "iopub.status.idle": "2024-02-17T17:20:54.539516Z",
     "shell.execute_reply": "2024-02-17T17:20:54.539000Z",
     "shell.execute_reply.started": "2024-02-17T17:20:54.535950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 640\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eed544-992d-40f6-9bb9-2a35d0aba654",
   "metadata": {},
   "source": [
    "### 如果全量训练，则使用完整数据代替抽样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c5819d9-d378-4020-acf0-f135d95987f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:57.848080Z",
     "iopub.status.busy": "2024-02-17T17:20:57.847737Z",
     "iopub.status.idle": "2024-02-17T17:20:57.911304Z",
     "shell.execute_reply": "2024-02-17T17:20:57.910765Z",
     "shell.execute_reply.started": "2024-02-17T17:20:57.848061Z"
    }
   },
   "outputs": [],
   "source": [
    "# 抽样数据处理\n",
    "tokenized_common_voice = small_common_voice.map(prepare_dataset)\n",
    "\n",
    "# 完整数据训练，尝试开启 `num_proc=8` 参数多进程并行处理（如阻塞无法运行，则不使用此参数）\n",
    "# tokenized_common_voice = common_voice.map(prepare_dataset, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "344eea6b-2753-4d7d-8f9d-0e766c5764df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:20:58.718866Z",
     "iopub.status.busy": "2024-02-17T17:20:58.718476Z",
     "iopub.status.idle": "2024-02-17T17:20:58.722433Z",
     "shell.execute_reply": "2024-02-17T17:20:58.721926Z",
     "shell.execute_reply.started": "2024-02-17T17:20:58.718846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
       "        num_rows: 640\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880da3c-576d-4dc3-b803-26eca162721d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:08:47.373013Z",
     "iopub.status.busy": "2024-02-17T17:08:47.372622Z",
     "iopub.status.idle": "2024-02-17T17:08:47.376999Z",
     "shell.execute_reply": "2024-02-17T17:08:47.376242Z",
     "shell.execute_reply.started": "2024-02-17T17:08:47.372990Z"
    }
   },
   "source": [
    "## 自定义语音数据整理器\n",
    "定义了一个针对语音到文本（Seq2Seq）模型的自定义数据整理器类，特别适用于输入为语音特征、输出为文本序列的数据集。\n",
    "\n",
    "这个整理器（DataCollatorSpeechSeq2SeqWithPadding）旨在将数据点批量打包，将每个批次中的attention_mask填充到最大长度，以保持批处理中张量形状的一致性，并用-100替换填充值，以便在损失函数中被忽略。这对于神经网络的高效训练至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c89ffcf-c805-48c2-b7d3-ae01b687178c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:21:00.904109Z",
     "iopub.status.busy": "2024-02-17T17:21:00.903745Z",
     "iopub.status.idle": "2024-02-17T17:21:00.910089Z",
     "shell.execute_reply": "2024-02-17T17:21:00.909520Z",
     "shell.execute_reply.started": "2024-02-17T17:21:00.904089Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# 定义一个针对语音到文本任务的数据整理器类\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # 处理器结合了特征提取器和分词器\n",
    "\n",
    "    # 整理器函数，将特征列表处理成一个批次\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 从特征列表中提取输入特征，并填充以使它们具有相同的形状\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 从特征列表中提取标签特征（文本令牌），并进行填充\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 使用-100替换标签中的填充区域，-100通常用于在损失计算中忽略填充令牌\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # 如果批次中的所有序列都以句子开始令牌开头，则移除它\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # 将处理过的标签添加到批次中\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch  # 返回最终的批次，准备好进行训练或评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f813f08-5a5c-4538-a08b-7c363c032a3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:21:01.597309Z",
     "iopub.status.busy": "2024-02-17T17:21:01.596957Z",
     "iopub.status.idle": "2024-02-17T17:21:01.600151Z",
     "shell.execute_reply": "2024-02-17T17:21:01.599639Z",
     "shell.execute_reply.started": "2024-02-17T17:21:01.597290Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用给定的处理器实例化数据整理器\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ecd4bc-01fd-4286-afe5-fe2639ae15a1",
   "metadata": {},
   "source": [
    "## 模型准备\n",
    "### 加载预训练模型（int8 精度）\n",
    "使用 int8  精度加载预训练模型，进一步降低显存需求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd09dc05-cbab-48dc-a0de-918b40ff54f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:21:03.958026Z",
     "iopub.status.busy": "2024-02-17T17:21:03.957656Z",
     "iopub.status.idle": "2024-02-17T17:21:06.910824Z",
     "shell.execute_reply": "2024-02-17T17:21:06.910249Z",
     "shell.execute_reply.started": "2024-02-17T17:21:03.957989Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33b43c94-5481-4a2c-8e84-99fb9cf361ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:21:06.912536Z",
     "iopub.status.busy": "2024-02-17T17:21:06.912087Z",
     "iopub.status.idle": "2024-02-17T17:21:06.915562Z",
     "shell.execute_reply": "2024-02-17T17:21:06.915013Z",
     "shell.execute_reply.started": "2024-02-17T17:21:06.912507Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置模型配置中的forced_decoder_ids属性为None\n",
    "model.config.forced_decoder_ids = None  # 这通常用于指定在解码（生成文本）过程中必须使用的特定token的ID，设置为None表示没有这样的强制要求\n",
    "\n",
    "# 设置模型配置中的suppress_tokens列表为空\n",
    "model.config.suppress_tokens = []  # 这用于指定在生成过程中应被抑制（不生成）的token的列表，设置为空列表表示没有要抑制的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355870af-233c-4f94-913c-092b4f614f4a",
   "metadata": {},
   "source": [
    "### PEFT 微调前的模型处理\n",
    "在使用 peft 训练 int8 模型之前，需要进行一些预处理：\n",
    "\n",
    "将所有非 int8 精度模块转换为全精度（fp32）以保证稳定性\n",
    "为输入嵌入层添加一个 forward_hook，以启用输入隐藏状态的梯度计算\n",
    "启用梯度检查点以实现更高效的内存训练\n",
    "使用 peft 库预定义的工具函数 prepare_model_for_int8_training，便可自动完成以上模型处理工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddfcc2ec-d1ba-4f12-a492-a334b3e11899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:21:11.943472Z",
     "iopub.status.busy": "2024-02-17T17:21:11.943048Z",
     "iopub.status.idle": "2024-02-17T17:21:11.971940Z",
     "shell.execute_reply": "2024-02-17T17:21:11.971405Z",
     "shell.execute_reply.started": "2024-02-17T17:21:11.943453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "model = prepare_model_for_int8_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34670fad-39b4-4c40-b7ba-44f10e6d9789",
   "metadata": {},
   "source": [
    "### LoRA Adapter 配置\n",
    "在 peft 中使用LoRA非常简捷，借助 PeftModel抽象，我们可以快速使用低秩适配器（LoRA）到任意模型。\n",
    "\n",
    "通过使用 peft 中的 get_peft_model 工具函数来实现。\n",
    "\n",
    "关于 LoRA 超参数的说明：\n",
    "MatMul(B,A) * Scaling\n",
    "Scaling = LoRA_Alpha / Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c32b8ab4-090e-4711-b945-ffad4f45cca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:21:23.788894Z",
     "iopub.status.busy": "2024-02-17T17:21:23.788532Z",
     "iopub.status.idle": "2024-02-17T17:21:23.792203Z",
     "shell.execute_reply": "2024-02-17T17:21:23.791698Z",
     "shell.execute_reply.started": "2024-02-17T17:21:23.788875Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "# 创建一个LoraConfig对象，用于设置LoRA（Low-Rank Adaptation）的配置参数\n",
    "config = LoraConfig(\n",
    "    r=8,  # LoRA的秩，影响LoRA矩阵的大小\n",
    "    lora_alpha=64,  # LoRA适应的比例因子\n",
    "    # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影。\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,  # 在LoRA模块中使用的dropout率\n",
    "    bias=\"none\",  # 设置bias的使用方式，这里没有使用bias\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3c4f1-82b6-498d-809f-3414aba2a3f8",
   "metadata": {},
   "source": [
    "### 使用get_peft_model函数和给定的配置来获取一个PEFT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e8cca91-bfcf-4f01-a953-1427b8e1a5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:21:25.604709Z",
     "iopub.status.busy": "2024-02-17T17:21:25.604350Z",
     "iopub.status.idle": "2024-02-17T17:21:25.766956Z",
     "shell.execute_reply": "2024-02-17T17:21:25.766396Z",
     "shell.execute_reply.started": "2024-02-17T17:21:25.604690Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb7f36ff-f4a6-4560-99af-59d9b8114488",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:21:26.949628Z",
     "iopub.status.busy": "2024-02-17T17:21:26.949244Z",
     "iopub.status.idle": "2024-02-17T17:21:26.960029Z",
     "shell.execute_reply": "2024-02-17T17:21:26.959487Z",
     "shell.execute_reply.started": "2024-02-17T17:21:26.949609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,932,160 || all params: 1,547,237,120 || trainable%: 0.25414074863974306\n"
     ]
    }
   ],
   "source": [
    "# 打印 LoRA 微调训练的模型参数\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a09fd1-20a5-45ec-b30f-0cef585bc842",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:11:48.765778Z",
     "iopub.status.busy": "2024-02-17T17:11:48.765424Z",
     "iopub.status.idle": "2024-02-17T17:11:48.769564Z",
     "shell.execute_reply": "2024-02-17T17:11:48.768928Z",
     "shell.execute_reply.started": "2024-02-17T17:11:48.765759Z"
    }
   },
   "source": [
    "## 模型训练\n",
    "### Seq2SeqTrainingArguments 训练参数\n",
    "关于设置训练步数和评估步数\n",
    "\n",
    "基于 epochs 设置：\n",
    "\n",
    "num_train_epochs=3,  # 训练的总轮数\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略，这里是在每个epoch结束时进行评估\n",
    "    warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "基于 steps 设置：\n",
    "\n",
    "evaluation_strategy=\"steps\", \n",
    "    eval_steps=25,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b47432af-ab07-4f26-9a4c-2ac866b7f24d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:12:01.064040Z",
     "iopub.status.busy": "2024-02-17T17:12:01.063604Z",
     "iopub.status.idle": "2024-02-17T17:12:01.083739Z",
     "shell.execute_reply": "2024-02-17T17:12:01.083092Z",
     "shell.execute_reply.started": "2024-02-17T17:12:01.064020Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 设置序列到序列模型训练的参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批量大小\n",
    "    learning_rate=1e-3,  # 学习率\n",
    "    num_train_epochs=1,  # 训练的总轮数\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略，这里是在每个epoch结束时进行评估\n",
    "    # warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "    # fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    per_device_eval_batch_size=batch_size,  # 每个设备上的评估批量大小\n",
    "    generation_max_length=128,  # 生成任务的最大长度\n",
    "    logging_steps=10,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733a1ec-77be-469a-b2e1-072988d66e9a",
   "metadata": {},
   "source": [
    "### 实例化 Seq2SeqTrainer 训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1673977d-58e5-4bda-91d5-99011e0c49a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:12:17.754675Z",
     "iopub.status.busy": "2024-02-17T17:12:17.754237Z",
     "iopub.status.idle": "2024-02-17T17:12:17.942768Z",
     "shell.execute_reply": "2024-02-17T17:12:17.942104Z",
     "shell.execute_reply.started": "2024-02-17T17:12:17.754654Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32e61a98-18c5-4140-b85b-6653d1c2f811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:12:24.834946Z",
     "iopub.status.busy": "2024-02-17T17:12:24.834385Z",
     "iopub.status.idle": "2024-02-17T17:18:03.326830Z",
     "shell.execute_reply": "2024-02-17T17:18:03.326313Z",
     "shell.execute_reply.started": "2024-02-17T17:12:24.834925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 05:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.494500</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=1.4945281982421874, metrics={'train_runtime': 338.2775, 'train_samples_per_second': 1.892, 'train_steps_per_second': 0.03, 'total_flos': 1.362453331968e+18, 'train_loss': 1.4945281982421874, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66008b-ceff-4ca8-ae20-1d0668bb05d0",
   "metadata": {},
   "source": [
    "### 保存 LoRA 模型(Adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b699aa0-3b67-4f7a-9776-1fdf8aa7f2d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:18:03.331360Z",
     "iopub.status.busy": "2024-02-17T17:18:03.331079Z",
     "iopub.status.idle": "2024-02-17T17:18:03.444195Z",
     "shell.execute_reply": "2024-02-17T17:18:03.443581Z",
     "shell.execute_reply.started": "2024-02-17T17:18:03.331343Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21682844-dd2c-4822-8b3b-c64e5da0eb0c",
   "metadata": {},
   "source": [
    "## 模型推理（可能需要重启 Notebook）\n",
    "再次加载模型会额外占用显存，如果显存已经达到上限，建议重启 Notebook 后再进行以下操作\n",
    "\n",
    "### 使用 PeftModel 加载 LoRA 微调后 Whisper 模型\n",
    "使用 PeftConfig 加载 LoRA Adapter 配置参数，使用 PeftModel 加载微调后 Whisper 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511ce2f-6d27-4ba5-b709-e16edf0fb725",
   "metadata": {},
   "source": [
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "language_decode = \"chinese\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e027b4a-9a68-423b-a330-5cf7120e23c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:18:28.464186Z",
     "iopub.status.busy": "2024-02-17T17:18:28.463957Z",
     "iopub.status.idle": "2024-02-17T17:18:28.471934Z",
     "shell.execute_reply": "2024-02-17T17:18:28.470671Z",
     "shell.execute_reply.started": "2024-02-17T17:18:28.464166Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "language_decode = \"chinese\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3131c8-926c-4a14-b80d-c867824018fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:19:03.681299Z",
     "iopub.status.busy": "2024-02-17T17:19:03.680929Z",
     "iopub.status.idle": "2024-02-17T17:19:09.836280Z",
     "shell.execute_reply": "2024-02-17T17:19:09.835704Z",
     "shell.execute_reply.started": "2024-02-17T17:19:03.681279Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoProcessor\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained(model_dir)\n",
    "\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(base_model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c264ee-4723-4c4c-bdfc-c64712f4e28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:19:09.837884Z",
     "iopub.status.busy": "2024-02-17T17:19:09.837381Z",
     "iopub.status.idle": "2024-02-17T17:19:11.015770Z",
     "shell.execute_reply": "2024-02-17T17:19:11.015208Z",
     "shell.execute_reply.started": "2024-02-17T17:19:09.837856Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "processor = AutoProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9232566-bbcc-413b-a0a9-e6a5c3821722",
   "metadata": {},
   "source": [
    "### 使用 Pipeline API 部署微调后 Whisper 实现中文语音识别任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36af4c46-fbc5-4ee5-8412-6f1270746de6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:19:12.942522Z",
     "iopub.status.busy": "2024-02-17T17:19:12.942092Z",
     "iopub.status.idle": "2024-02-17T17:19:12.945603Z",
     "shell.execute_reply": "2024-02-17T17:19:12.944955Z",
     "shell.execute_reply.started": "2024-02-17T17:19:12.942498Z"
    }
   },
   "outputs": [],
   "source": [
    "test_audio = \"data/audio/test_zh.flac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c3fc2f-9583-4275-83ab-7b917f3a3f8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:19:13.820398Z",
     "iopub.status.busy": "2024-02-17T17:19:13.820028Z",
     "iopub.status.idle": "2024-02-17T17:19:13.912008Z",
     "shell.execute_reply": "2024-02-17T17:19:13.911513Z",
     "shell.execute_reply.started": "2024-02-17T17:19:13.820379Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModel' is not supported for . Supported models are ['Pop2PianoForConditionalGeneration', 'SeamlessM4TForSpeechToText', 'SeamlessM4Tv2ForSpeechToText', 'SpeechEncoderDecoderModel', 'Speech2TextForConditionalGeneration', 'SpeechT5ForSpeechToText', 'WhisperForConditionalGeneration', 'Data2VecAudioForCTC', 'HubertForCTC', 'MCTCTForCTC', 'SEWForCTC', 'SEWDForCTC', 'UniSpeechForCTC', 'UniSpeechSatForCTC', 'Wav2Vec2ForCTC', 'Wav2Vec2ConformerForCTC', 'WavLMForCTC'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=peft_model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_decode, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30661f96-bc56-4fb6-95ae-9865f83a8d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:19:15.379964Z",
     "iopub.status.busy": "2024-02-17T17:19:15.379426Z",
     "iopub.status.idle": "2024-02-17T17:19:16.088370Z",
     "shell.execute_reply": "2024-02-17T17:19:16.087595Z",
     "shell.execute_reply.started": "2024-02-17T17:19:15.379943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ffmpeg was not found but is required to load audio files from filename",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/audio_utils.py:34\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[0;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffmpeg_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ffmpeg_process:\n\u001b[1;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m----> 4\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:357\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    296\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    298\u001b[0m ):\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/base.py:1132\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:32\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:183\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:434\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.preprocess\u001b[0;34m(self, inputs, chunk_length_s, stride_length_s)\u001b[0m\n\u001b[1;32m    431\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m--> 434\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    437\u001b[0m extra \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.10/site-packages/transformers/pipelines/audio_utils.py:37\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[0;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg was not found but is required to load audio files from filename\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n\u001b[1;32m     38\u001b[0m out_bytes \u001b[38;5;241m=\u001b[39m output_stream[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(out_bytes, np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mValueError\u001b[0m: ffmpeg was not found but is required to load audio files from filename"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(test_audio, max_new_tokens=255)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3ca6757-aecd-4ff3-ad33-a619e85f3b9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:19:26.382595Z",
     "iopub.status.busy": "2024-02-17T17:19:26.382151Z",
     "iopub.status.idle": "2024-02-17T17:19:26.397885Z",
     "shell.execute_reply": "2024-02-17T17:19:26.397229Z",
     "shell.execute_reply.started": "2024-02-17T17:19:26.382576Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtext\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a57407-5fdc-4438-8679-469c43db240b",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397fc10-4f61-417f-a282-e889931117ff",
   "metadata": {},
   "source": [
    "### 使用完整的数据集训练，对比 Train Loss 和 Validation Loss 变化。训练完成后，使用测试集进行模型评估."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea45924f-9603-4688-b693-79bec50ab1f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T17:27:03.655490Z",
     "iopub.status.busy": "2024-02-17T17:27:03.655113Z",
     "iopub.status.idle": "2024-02-17T18:54:21.693289Z",
     "shell.execute_reply": "2024-02-17T18:54:21.692774Z",
     "shell.execute_reply.started": "2024-02-17T17:27:03.655470Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29056/29056 [1:03:35<00:00,  7.62 examples/s]  \n",
      "Map: 100%|██████████| 10581/10581 [23:42<00:00,  7.44 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "tokenized_common_voice = common_voice.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05f6b751-b473-4b18-8d5b-161204fdefab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T19:06:25.803969Z",
     "iopub.status.busy": "2024-02-17T19:06:25.803530Z",
     "iopub.status.idle": "2024-02-17T19:06:25.809696Z",
     "shell.execute_reply": "2024-02-17T19:06:25.809183Z",
     "shell.execute_reply.started": "2024-02-17T19:06:25.803949Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 设置序列到序列模型训练的参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批量大小\n",
    "    learning_rate=1e-3,  # 学习率\n",
    "    num_train_epochs=2,  # 训练的总轮数\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略，这里是在每个epoch结束时进行评估\n",
    "    # warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "    # fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    per_device_eval_batch_size=batch_size,  # 每个设备上的评估批量大小\n",
    "    generation_max_length=128,  # 生成任务的最大长度\n",
    "    logging_steps=10,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f869efbf-2f58-446a-95df-7695ef2250cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T19:06:28.718721Z",
     "iopub.status.busy": "2024-02-17T19:06:28.718358Z",
     "iopub.status.idle": "2024-02-17T19:06:28.723974Z",
     "shell.execute_reply": "2024-02-17T19:06:28.723467Z",
     "shell.execute_reply.started": "2024-02-17T19:06:28.718701Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0267e37-d3bf-45b0-80e3-ced8b40cfced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-17T19:06:29.818796Z",
     "iopub.status.busy": "2024-02-17T19:06:29.818451Z",
     "iopub.status.idle": "2024-02-18T02:53:56.598688Z",
     "shell.execute_reply": "2024-02-18T02:53:56.597172Z",
     "shell.execute_reply.started": "2024-02-17T19:06:29.818777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='908' max='908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [908/908 7:46:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.387619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.376251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=908, training_loss=0.30537255193693524, metrics={'train_runtime': 28046.3957, 'train_samples_per_second': 2.072, 'train_steps_per_second': 0.032, 'total_flos': 1.237107625426944e+20, 'train_loss': 0.30537255193693524, 'epoch': 2.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b5179c2-e4cd-4192-993b-0b724c8ba76a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T03:44:35.390146Z",
     "iopub.status.busy": "2024-02-18T03:44:35.389745Z",
     "iopub.status.idle": "2024-02-18T03:44:35.513553Z",
     "shell.execute_reply": "2024-02-18T03:44:35.512985Z",
     "shell.execute_reply.started": "2024-02-18T03:44:35.390127Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10545643-19b9-43e8-9c3a-493426aacf58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-18T03:44:37.072378Z",
     "iopub.status.busy": "2024-02-18T03:44:37.071891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/166 37:49 < 15:03, 0.05 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_common_voice[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f240181-ee41-49e2-aaa6-5c0878bbcd89",
   "metadata": {},
   "source": [
    "### [Optional]使用其他语种（如：德语、法语等）的数据集进行微调训练，并进行模型评估模型评估。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
